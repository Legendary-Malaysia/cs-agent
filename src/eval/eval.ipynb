{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea47b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csagent.supervisor.graph import supervisor_graph\n",
    "from utils import run_langsmith_eval\n",
    "from csagent.configuration import Configuration\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Literal\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6df3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = Configuration()\n",
    "\n",
    "\n",
    "def target_function(config: Configuration):\n",
    "    \"\"\"\n",
    "    Target function for Supervisor Graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def eval_supervisor(inputs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Eval Supervisor Graph.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = supervisor_graph.invoke(\n",
    "                inputs,\n",
    "                context=config,\n",
    "            )\n",
    "            time.sleep(15)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in eval_supervisor: {e}\")\n",
    "            return {}\n",
    "\n",
    "    return eval_supervisor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationModel(BaseModel):\n",
    "    \"\"\"Structured output of the evaluation.\"\"\"\n",
    "\n",
    "    rationale: str = Field(\n",
    "        description=\"Rationale that explains the alignment between the AI output and the ground truth.\"\n",
    "    )\n",
    "    score: Literal[0, 1] = Field(\n",
    "        description=\"0 means the AI output is completely wrong and not related to the ground truth, 1 means the AI output is completely correct and aligned with the ground truth.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_judge(inputs: str, ai_output: str, ground_truth: str, model: str):\n",
    "    \"\"\"Judge the AI output based on the ground truth.\"\"\"\n",
    "\n",
    "    instructions = \"\"\"\n",
    "        You are given a human question and a pair consisting of a ground truth and an AI-generated output. Your task is to evaluate how well the AI output aligns with the ground truth in the context of the human question.\n",
    "        1. Provide a brief reasoning (1-2 sentences) explaining the degree of alignment between the AI output and the ground truth.\n",
    "        2. Assign a binary score:\n",
    "            - 1 if the AI output aligns with the ground truth.\n",
    "            - 0 if the AI output does not align with the ground truth.\n",
    "        Keep your reasoning concise, objective, and focused only on the alignment. Do not add extra commentary, suggestions, or subjective opinions.\n",
    "\n",
    "        Human Question: {inputs}\n",
    "        AI output: {ai_output}\n",
    "        Ground truth: {ground_truth}\n",
    "\n",
    "        Format Instruction:\n",
    "        {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=EvaluationModel)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", instructions),\n",
    "        ]\n",
    "    ).partial(\n",
    "        inputs=inputs,\n",
    "        ai_output=ai_output,\n",
    "        ground_truth=ground_truth,\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "    )\n",
    "    try:\n",
    "        llm = init_chat_model(model, temperature=0)\n",
    "\n",
    "        response = llm.invoke(chat_prompt.invoke({}))\n",
    "        # Extract text content from AIMessage before parsing\n",
    "        response = parser.parse(response.content)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to judge AI output: %s\", str(e))\n",
    "        return EvaluationModel(rationale=\"Error. Failed to judge AI output\", score=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_alignment_evaluator(\n",
    "    inputs: dict, outputs: dict, reference_outputs: dict\n",
    ") -> list:\n",
    "    \"\"\"LLM-as-judge alignment evaluator.\"\"\"\n",
    "\n",
    "    try:\n",
    "        input_content = inputs[\"messages\"][-1][\"content\"]\n",
    "        output_content = outputs[\"messages\"][-1].content\n",
    "        reference_content = reference_outputs[\"content\"]\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logger.error(\"Invalid data structure in evaluator: %s\", e)\n",
    "        return [\n",
    "            {\"key\": \"alignment_score\", \"score\": 0},\n",
    "            {\n",
    "                \"key\": \"alignment_reasoning\",\n",
    "                \"value\": f\"Error: Invalid data structure - {e}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    response_alignment = llm_judge(\n",
    "        input_content,\n",
    "        output_content,\n",
    "        reference_content,\n",
    "        configuration.model_small,\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"key\": \"alignment_score\",\n",
    "            \"score\": response_alignment.score,\n",
    "        },\n",
    "        {\"key\": \"alignment_reasoning\", \"value\": response_alignment.rationale},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_langsmith_eval(\n",
    "    target_function(config=configuration),\n",
    "    \"CS Agent Evaluation\",\n",
    "    [\n",
    "        llm_alignment_evaluator,\n",
    "    ],\n",
    "    configuration.model,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

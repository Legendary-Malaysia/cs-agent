{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea47b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csagent.supervisor.graph import supervisor_graph\n",
    "from csagent.router_agent.graph import router_graph\n",
    "from csagent.react_agent.graph import react_agent_graph\n",
    "from utils import run_langsmith_eval\n",
    "from csagent.configuration import Configuration\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langdetect import detect, DetectorFactory\n",
    "from typing import Literal\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6df3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"id\"  # \"en\" #\n",
    "\n",
    "configuration = Configuration(\n",
    "    model=\"LongCat-Flash-Chat\",\n",
    "    model_medium=\"GLM-4.6V-Flash\",\n",
    "    model_small=\"google_genai:gemma-3-12b-it\",\n",
    "    language=language,\n",
    ")\n",
    "\n",
    "\n",
    "def create_target_function(graph, graph_name: str):\n",
    "    \"\"\"\n",
    "    Factory function to create target functions for any graph.\n",
    "\n",
    "    Args:\n",
    "        graph: The LangGraph graph to invoke\n",
    "        graph_name: Name of the graph for logging purposes\n",
    "    \"\"\"\n",
    "\n",
    "    def eval_graph(config: Configuration):\n",
    "        def invoke_graph(inputs: dict) -> dict:\n",
    "            \"\"\"\n",
    "            Evaluate the graph with given inputs.\n",
    "            \"\"\"\n",
    "            final_result = {}\n",
    "            tools_called = []\n",
    "            try:\n",
    "                # result = graph.invoke(\n",
    "                #     inputs,\n",
    "                #     context=config,\n",
    "                # )\n",
    "                # return result\n",
    "\n",
    "                for namespace, chunk in graph.stream(\n",
    "                    inputs,\n",
    "                    subgraphs=True,\n",
    "                    stream_mode=\"debug\",\n",
    "                    context=config,\n",
    "                ):\n",
    "                    # print(namespace, chunk)\n",
    "                    if not namespace:\n",
    "                        payload = chunk.get(\"payload\") or {}\n",
    "                        result = payload.get(\"result\") or {}\n",
    "                        messages = result.get(\"messages\")\n",
    "\n",
    "                        if messages:\n",
    "                            # Record final response\n",
    "                            final_result[\"messages\"] = messages\n",
    "                            # print(\"response: \", messages)\n",
    "\n",
    "                    if chunk.get(\"type\") == \"task\":\n",
    "                        payload = chunk.get(\"payload\", {})\n",
    "                        if payload.get(\"name\") == \"tools\":\n",
    "                            # Record tool calls\n",
    "                            tool_call = payload.get(\"input\", {}).get(\"tool_call\")\n",
    "                            if tool_call:\n",
    "                                tools_called.append(\n",
    "                                    {\n",
    "                                        tool_call.get(\n",
    "                                            \"name\", \"tool_name\"\n",
    "                                        ): tool_call.get(\"args\", {})\n",
    "                                    }\n",
    "                                )\n",
    "                                # print(\"tool_called: \", {tool_call[\"name\"]: tool_call[\"args\"]})\n",
    "\n",
    "                final_result[\"tools_called\"] = tools_called\n",
    "                # print(\"final_result\", final_result)\n",
    "                time.sleep(15)\n",
    "                return final_result\n",
    "\n",
    "            except Exception:\n",
    "                logger.exception(f\"Error in {graph_name}\")\n",
    "                return {}\n",
    "\n",
    "        return invoke_graph\n",
    "\n",
    "    return eval_graph\n",
    "\n",
    "\n",
    "# Usage:\n",
    "target_function = create_target_function(supervisor_graph, \"supervisor_graph\")\n",
    "target_function_router = create_target_function(router_graph, \"router_graph\")\n",
    "target_function_react = create_target_function(react_agent_graph, \"react_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f274217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = target_function(config=Configuration())(\n",
    "# response = target_function_router(config=Configuration())(\n",
    "# response = target_function_react(config=Configuration())(\n",
    "#     # {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about orchid\"}]}\n",
    "#     # {\"messages\": [{\"role\": \"user\", \"content\": \"Where can I find a product in penang?\"}]}\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": \"What is Orchid? Where can I find a product in penang?\",\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationModel(BaseModel):\n",
    "    \"\"\"Structured output of the evaluation.\"\"\"\n",
    "\n",
    "    rationale: str = Field(\n",
    "        description=\"Rationale that explains the alignment between the AI output and the ground truth.\"\n",
    "    )\n",
    "    score: Literal[0, 1] = Field(\n",
    "        description=\"0 means the AI output is completely wrong and not related to the ground truth, 1 means the AI output is completely correct and aligned with the ground truth.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_judge(inputs: str, ai_output: str, ground_truth: str, model: str):\n",
    "    \"\"\"Judge the AI output based on the ground truth.\"\"\"\n",
    "\n",
    "    instructions = \"\"\"\n",
    "        You are given a human question and a pair consisting of a ground truth and an AI-generated output. Your task is to evaluate how well the AI output aligns with the ground truth in the context of the human question.\n",
    "        1. Provide a brief reasoning (1-2 sentences) explaining the degree of alignment between the AI output and the ground truth.\n",
    "        2. Assign a binary score:\n",
    "            - 1 if the AI output aligns with the ground truth.\n",
    "            - 0 if the AI output does not align with the ground truth.\n",
    "        Keep your reasoning concise, objective, and focused only on the alignment. Do not add extra commentary, suggestions, or subjective opinions.\n",
    "\n",
    "        Human Question: {inputs}\n",
    "        AI output: {ai_output}\n",
    "        Ground truth: {ground_truth}\n",
    "\n",
    "        Format Instruction:\n",
    "        {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=EvaluationModel)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", instructions),\n",
    "        ]\n",
    "    ).partial(\n",
    "        inputs=inputs,\n",
    "        ai_output=ai_output,\n",
    "        ground_truth=ground_truth,\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "    )\n",
    "    try:\n",
    "        llm = init_chat_model(model, temperature=0)\n",
    "\n",
    "        response = llm.invoke(chat_prompt.invoke({}))\n",
    "        # Extract text content from AIMessage before parsing\n",
    "        response = parser.parse(response.content)\n",
    "        return response\n",
    "    except Exception:\n",
    "        logger.exception(\"Failed to judge AI output.\")\n",
    "        return EvaluationModel(rationale=\"Error. Failed to judge AI output\", score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_alignment_evaluator(\n",
    "    inputs: dict, outputs: dict, reference_outputs: dict\n",
    ") -> list:\n",
    "    \"\"\"LLM-as-judge alignment evaluator.\"\"\"\n",
    "\n",
    "    try:\n",
    "        input_content = inputs[\"messages\"][-1][\"content\"]\n",
    "        output_content = outputs[\"messages\"][-1].content\n",
    "        reference_content = reference_outputs[\"content\"]\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logger.exception(\"Invalid data structure in evaluator.\")\n",
    "        return [\n",
    "            {\"key\": \"alignment_score\", \"score\": 0},\n",
    "            {\n",
    "                \"key\": \"alignment_reasoning\",\n",
    "                \"value\": f\"Error: Invalid data structure - {e}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    response_alignment = llm_judge(\n",
    "        input_content,\n",
    "        output_content,\n",
    "        reference_content,\n",
    "        configuration.model_small,\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"key\": \"alignment_score\",\n",
    "            \"score\": response_alignment.score,\n",
    "        },\n",
    "        {\"key\": \"alignment_reasoning\", \"value\": response_alignment.rationale},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Check whether AI output and reference outputs are in the same language.\"\"\"\n",
    "\n",
    "    DetectorFactory.seed = 0  # Makes results reproducible\n",
    "    sentence1 = detect(outputs[\"messages\"][-1].content)\n",
    "    sentence2 = detect(reference_outputs[\"content\"])\n",
    "\n",
    "    is_same_language = sentence1 == sentence2\n",
    "\n",
    "    return {\n",
    "        \"key\": \"is_same_language\",\n",
    "        \"score\": is_same_language,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5982cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tools_called(expected_tools: list, actual_tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Compare expected tools with actual tools called.\n",
    "\n",
    "    Args:\n",
    "        expected_tools: List of expected tool calls with parameters\n",
    "        actual_tools: List of actual tool calls made\n",
    "\n",
    "    Returns:\n",
    "        dict: Results with match status, missing tools, extra tools, and parameter mismatches\n",
    "    \"\"\"\n",
    "\n",
    "    # Edge case: Handle None or empty inputs\n",
    "    if expected_tools is None:\n",
    "        expected_tools = []\n",
    "    if actual_tools is None:\n",
    "        actual_tools = []\n",
    "\n",
    "    # Normalize to lists if single dict provided\n",
    "    if isinstance(expected_tools, dict):\n",
    "        expected_tools = [expected_tools]\n",
    "    if isinstance(actual_tools, dict):\n",
    "        actual_tools = [actual_tools]\n",
    "\n",
    "    results = {\n",
    "        \"is_match\": True,\n",
    "        \"missing_tools\": [],\n",
    "        \"extra_tools\": [],\n",
    "        \"parameter_mismatches\": [],\n",
    "        \"details\": [],\n",
    "    }\n",
    "\n",
    "    # Create normalized versions for comparison\n",
    "    def normalize_tool(tool):\n",
    "        \"\"\"Extract tool name and parameters\"\"\"\n",
    "        if not tool:\n",
    "            return None\n",
    "        tool_name = list(tool.keys())[0]\n",
    "        params = tool[tool_name]\n",
    "        return (tool_name, params if isinstance(params, dict) else {})\n",
    "\n",
    "    expected_normalized = [normalize_tool(t) for t in expected_tools if t]\n",
    "    actual_normalized = [normalize_tool(t) for t in actual_tools if t]\n",
    "\n",
    "    # Track which actual tools have been matched\n",
    "    matched_actual = [False] * len(actual_normalized)\n",
    "\n",
    "    # Check each expected tool\n",
    "    for exp_tool_name, exp_params in expected_normalized:\n",
    "        found_match = False\n",
    "\n",
    "        for i, (act_tool_name, act_params) in enumerate(actual_normalized):\n",
    "            if matched_actual[i]:\n",
    "                continue\n",
    "\n",
    "            # Check if tool names match\n",
    "            if exp_tool_name == act_tool_name:\n",
    "                # Check if parameters match\n",
    "                param_match = True\n",
    "                mismatches = []\n",
    "\n",
    "                # Check all expected parameters\n",
    "                for key, expected_value in exp_params.items():\n",
    "                    actual_value = act_params.get(key)\n",
    "\n",
    "                    # Handle case-insensitive comparison for strings\n",
    "                    if isinstance(expected_value, str) and isinstance(\n",
    "                        actual_value, str\n",
    "                    ):\n",
    "                        if expected_value.lower() != actual_value.lower():\n",
    "                            param_match = False\n",
    "                            mismatches.append(\n",
    "                                {\n",
    "                                    \"tool\": exp_tool_name,\n",
    "                                    \"parameter\": key,\n",
    "                                    \"expected\": expected_value,\n",
    "                                    \"actual\": actual_value,\n",
    "                                }\n",
    "                            )\n",
    "                    elif expected_value != actual_value:\n",
    "                        param_match = False\n",
    "                        mismatches.append(\n",
    "                            {\n",
    "                                \"tool\": exp_tool_name,\n",
    "                                \"parameter\": key,\n",
    "                                \"expected\": expected_value,\n",
    "                                \"actual\": actual_value,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                # Check for unexpected parameters\n",
    "                for key in act_params:\n",
    "                    if key not in exp_params:\n",
    "                        mismatches.append(\n",
    "                            {\n",
    "                                \"tool\": exp_tool_name,\n",
    "                                \"parameter\": key,\n",
    "                                \"expected\": None,\n",
    "                                \"actual\": act_params[key],\n",
    "                                \"type\": \"unexpected_parameter\",\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                if param_match and not mismatches:\n",
    "                    found_match = True\n",
    "                    matched_actual[i] = True\n",
    "                    results[\"details\"].append(\n",
    "                        {\"tool\": exp_tool_name, \"status\": \"matched\"}\n",
    "                    )\n",
    "                    break\n",
    "                elif mismatches:\n",
    "                    results[\"parameter_mismatches\"].extend(mismatches)\n",
    "\n",
    "        if not found_match:\n",
    "            results[\"missing_tools\"].append({exp_tool_name: exp_params})\n",
    "            results[\"is_match\"] = False\n",
    "\n",
    "    # Check for extra tools that weren't expected\n",
    "    for i, (act_tool_name, act_params) in enumerate(actual_normalized):\n",
    "        if not matched_actual[i]:\n",
    "            results[\"extra_tools\"].append({act_tool_name: act_params})\n",
    "            results[\"is_match\"] = False\n",
    "\n",
    "    # Final match determination\n",
    "    if (\n",
    "        results[\"missing_tools\"]\n",
    "        or results[\"extra_tools\"]\n",
    "        or results[\"parameter_mismatches\"]\n",
    "    ):\n",
    "        results[\"is_match\"] = False\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def is_correct_tool_called(\n",
    "    inputs: dict, outputs: dict, reference_outputs: dict\n",
    ") -> list:\n",
    "    \"\"\"Check if the number of tools called is correct.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # input_content = inputs[\"messages\"][-1][\"content\"]\n",
    "        output_tools_called = outputs[\"tools_called\"]\n",
    "        reference_tools_called = reference_outputs[\"expected_tools_called\"]\n",
    "        result = check_tools_called(reference_tools_called, output_tools_called)\n",
    "    except (KeyError, IndexError):\n",
    "        logger.exception(\"Invalid data structure in tool_call_number_evaluator.\")\n",
    "        return {\"key\": \"tool_count\", \"score\": 0}\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"key\": \"is_correct_tool_called\",\n",
    "            \"score\": result[\"is_match\"],\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"missing_tools\",\n",
    "            \"value\": json.dumps(result[\"missing_tools\"]),\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"extra_tools\",\n",
    "            \"value\": json.dumps(result[\"extra_tools\"]),\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"parameter_mismatches\",\n",
    "            \"value\": json.dumps(result[\"parameter_mismatches\"]),\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which graph to evaluate\n",
    "EVAL_TARGET = \"react\"  # or \"router\"  # or \"supervisor\"\n",
    "target_mapping = {\n",
    "    \"react\": target_function_react,\n",
    "    \"router\": target_function_router,\n",
    "    \"supervisor\": target_function,\n",
    "}\n",
    "target = target_mapping[EVAL_TARGET]\n",
    "\n",
    "\n",
    "run_langsmith_eval(\n",
    "    target(config=configuration),\n",
    "    # \"CS Agent Evaluation\",\n",
    "    \"CS Agent Test Evaluation\",\n",
    "    [llm_alignment_evaluator, language_evaluator, is_correct_tool_called],\n",
    "    configuration.model,\n",
    "    split_name=language,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
